import requests
import time
import os
import zipfile
import json
import platform
import subprocess
from PIL import Image
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
from logger_config_ocr import get_logger

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger("ocr_api")

class OCRProcessor:
    def __init__(self, token, pdf_folder=None):
        self.token = token
        self.pdf_folder = pdf_folder
        self.session = self._create_session()
        self.headers = {
            'Authorization': f'Bearer {token}'
        }
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯"""
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504, 521, 522, 524],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session
    
    def convert_emf_to_png(self, emf_path, png_path):
        """
        å°†EMFæ–‡ä»¶è½¬æ¢ä¸ºPNGæ ¼å¼
        æ”¯æŒWindowså’ŒLinuxç³»ç»Ÿ
        """
        system = platform.system().lower()
        
        if system == 'windows':
            # Windowså¹³å°ä½¿ç”¨åŸæœ‰æ–¹æ³•
            try:
                # ä½¿ç”¨PILå°è¯•æ‰“å¼€EMFæ–‡ä»¶
                image = Image.open(emf_path)
                # ä¿å­˜ä¸ºPNGæ ¼å¼
                image.save(png_path, 'PNG')
                logger.info(f"âœ… EMFæ–‡ä»¶å·²è½¬æ¢ä¸ºPNG: {png_path}")
                return True
            except Exception as e:
                logger.error(f"âŒ EMFè½¬æ¢PNGå¤±è´¥: {e}")
                return False
        else:
            # Linux/Macå¹³å°ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
            # å°è¯•ä½¿ç”¨Inkscape
            if self._convert_emf_to_png_inkscape(emf_path, png_path):
                return True
            # å°è¯•ä½¿ç”¨LibreOffice
            elif self._convert_emf_to_png_libreoffice(emf_path, png_path):
                return True
            else:
                logger.error(f"âŒ åœ¨{system}ç³»ç»Ÿä¸Šæ— æ³•è½¬æ¢EMFæ–‡ä»¶: {emf_path}")
                return False
    
    def _convert_emf_to_png_inkscape(self, emf_path, png_path):
        """
        ä½¿ç”¨Inkscapeè½¬æ¢EMFåˆ°PNG
        éœ€è¦å®‰è£…: sudo apt-get install inkscape
        """
        try:
            cmd = [
                'inkscape',
                emf_path,
                '--export-type=png',
                f'--export-filename={png_path}'
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            logger.info(f"âœ… ä½¿ç”¨Inkscapeå°†EMFæ–‡ä»¶è½¬æ¢ä¸ºPNG: {png_path}")
            return True
        except (subprocess.CalledProcessError, FileNotFoundError) as e:
            logger.warning(f"âš ï¸ ä½¿ç”¨Inkscapeè½¬æ¢EMFå¤±è´¥: {e}")
            return False
    
    def _convert_emf_to_png_libreoffice(self, emf_path, png_path):
        """
        ä½¿ç”¨LibreOfficeè½¬æ¢EMFåˆ°PNG
        éœ€è¦å®‰è£…: sudo apt-get install libreoffice
        """
        try:
            # ä½¿ç”¨libreofficeå°†EMFè½¬æ¢ä¸ºPNG
            cmd = [
                'libreoffice',
                '--headless',
                '--convert-to', 'png',
                '--outdir', os.path.dirname(png_path),
                emf_path
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            # LibreOfficeä¼šè‡ªåŠ¨å‘½åè¾“å‡ºæ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦é‡å‘½åä¸ºç›®æ ‡æ–‡ä»¶å
            base_name = os.path.splitext(os.path.basename(emf_path))[0]
            auto_generated_path = os.path.join(os.path.dirname(png_path), f"{base_name}.png")
            if os.path.exists(auto_generated_path):
                os.rename(auto_generated_path, png_path)
                logger.info(f"âœ… ä½¿ç”¨LibreOfficeå°†EMFæ–‡ä»¶è½¬æ¢ä¸ºPNG: {png_path}")
                return True
            return False
        except (subprocess.CalledProcessError, FileNotFoundError) as e:
            logger.warning(f"âš ï¸ ä½¿ç”¨LibreOfficeè½¬æ¢EMFå¤±è´¥: {e}")
            return False
    
    def convert_emf_to_pdf(self, emf_path, pdf_path):
        """
        å°†EMFæ–‡ä»¶è½¬æ¢ä¸ºPDFæ ¼å¼
        """
        try:
            # å…ˆè½¬æ¢ä¸ºPNGï¼Œå†è½¬æ¢ä¸ºPDF
            image = Image.open(emf_path)
            # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if image.mode in ('RGBA', 'LA', 'P'):
                # åˆ›å»ºç™½è‰²èƒŒæ™¯
                background = Image.new('RGB', image.size, (255, 255, 255))
                if image.mode == 'P':
                    image = image.convert('RGBA')
                background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
                image = background
            
            # ä¿å­˜ä¸ºPDF
            image.save(pdf_path, 'PDF', resolution=100.0)
            logger.info(f"âœ… EMFæ–‡ä»¶å·²è½¬æ¢ä¸ºPDF: {pdf_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ EMFè½¬æ¢PDFå¤±è´¥: {e}")
            return False
    
    def upload_file(self, file_path):
        """ä¸Šä¼ æœ¬åœ°æ–‡ä»¶åˆ°ä¸´æ—¶å­˜å‚¨"""
        logger.info(f"ğŸ“¤ æ­£åœ¨ä¸Šä¼ æ–‡ä»¶: {os.path.basename(file_path)}")
        
        # ä½¿ç”¨ tmpfiles.org
        try:
            with open(file_path, 'rb') as f:
                response = self.session.post(
                    'https://tmpfiles.org/api/v1/upload',
                    files={'file': f},
                    timeout=(30, 60)
                )
                if response.status_code == 200:
                    result = response.json()
                    # è·å–ç›´æ¥ä¸‹è½½é“¾æ¥
                    url = result['data']['url']
                    direct_url = url.replace('tmpfiles.org/', 'tmpfiles.org/dl/')
                    logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {direct_url}")
                    return direct_url
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
        
        return None
    
    def process_pdf(self, file_path):
        """å¤„ç†æœ¬åœ°PDFæ–‡ä»¶"""
        # 1. ä¸Šä¼ æ–‡ä»¶
        pdf_url = self.upload_file(file_path)
        if not pdf_url:
            return None
        
        # 2. åˆ›å»ºMinerUä»»åŠ¡
        logger.info("ğŸ“„ åˆ›å»ºè§£æä»»åŠ¡...")
        task_url = 'https://mineru.net/api/v4/extract/task'
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.token}'
        }
        data = {
            'url': pdf_url,
            'is_ocr': True,
            'enable_formula': True,
            'enable_table': True,
            'language': 'auto'
        }
        
        try:
            response = self.session.post(
                task_url, 
                headers=headers, 
                json=data,
                timeout=(30, 60)
            )
            result = response.json()
            
            if result['code'] != 0:
                logger.error(f"âŒ åˆ›å»ºä»»åŠ¡å¤±è´¥: {result['msg']}")
                return None
            
            task_id = result['data']['task_id']
            logger.info(f"âœ… ä»»åŠ¡ID: {task_id}")
            
            # 3. ç­‰å¾…å¤„ç†å®Œæˆ
            logger.info("â³ ç­‰å¾…å¤„ç†...")
            return self._wait_for_task_completion(task_id, headers)
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºä»»åŠ¡æ—¶å‡ºé”™: {e}")
            return None
    
    def _wait_for_task_completion(self, task_id, headers):
        """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
        task_url = f'https://mineru.net/api/v4/extract/task/{task_id}'
        while True:
            try:
                time.sleep(5)
                status_response = self.session.get(
                    task_url, 
                    headers=headers,
                    timeout=(30, 60)
                )
                status_data = status_response.json()
                
                state = status_data['data']['state']
                
                if state == 'done':
                    zip_url = status_data['data']['full_zip_url']
                    logger.info(f"âœ… å¤„ç†å®Œæˆï¼")
                    logger.info(f"ğŸ“¦ ä¸‹è½½åœ°å€: {zip_url}")
                    
                    # ä¸‹è½½ç»“æœ
                    self.download_result(zip_url, task_id)
                    return status_data
                    
                elif state == 'failed':
                    logger.error(f"âŒ å¤„ç†å¤±è´¥: {status_data['data']['err_msg']}")
                    return None
                    
                elif state == 'running':
                    progress = status_data['data'].get('extract_progress', {})
                    extracted = progress.get('extracted_pages', 0)
                    total = progress.get('total_pages', 0)
                    logger.info(f"â³ æ­£åœ¨å¤„ç†... {extracted}/{total} é¡µ")
                
                else:
                    logger.info(f"ğŸ“Š çŠ¶æ€: {state}")
            except Exception as e:
                logger.error(f"âŒ æ£€æŸ¥ä»»åŠ¡çŠ¶æ€æ—¶å‡ºé”™: {e}")
    
    def download_result(self, zip_url, task_id):
        """ä¸‹è½½ç»“æœæ–‡ä»¶"""
        save_path = f"mineru_result_{task_id}.zip"
        
        try:
            response = self.session.get(
                zip_url, 
                stream=True,
                timeout=(30, 300)
            )
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:  # è¿‡æ»¤æ‰keep-alive chunks
                        f.write(chunk)
            logger.info(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")

    def batch_process_pdfs(self, file_paths, data_ids=None):
        """æ‰¹é‡å¤„ç†PDFæ–‡ä»¶"""
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        valid_files = []
        valid_data_ids = []
        
        for i, file_path in enumerate(file_paths):
            if os.path.exists(file_path):
                valid_files.append(file_path)
                if data_ids and i < len(data_ids):
                    valid_data_ids.append(data_ids[i])
                else:
                    # å¦‚æœæ²¡æœ‰æä¾›data_idï¼Œåˆ™ä½¿ç”¨æ–‡ä»¶åä½œä¸ºdata_id
                    valid_data_ids.append(os.path.basename(file_path))
            else:
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not valid_files:
            logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯ä»¥å¤„ç†")
            return None
            
        # å‡†å¤‡æ–‡ä»¶ä¿¡æ¯
        files_info = []
        file_names = []
        for i, file_path in enumerate(valid_files):
            file_name = os.path.basename(file_path)
            file_names.append(file_name)
            files_info.append({
                "name": file_name,
                "is_ocr": True,
                "data_id": valid_data_ids[i]
            })
        
        # å‘é€æ‰¹é‡å¤„ç†è¯·æ±‚
        logger.info("ğŸ“„ ç”³è¯·æ‰¹é‡å¤„ç†...")
        batch_url = 'https://mineru.net/api/v4/file-urls/batch'
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.token}'
        }
        
        data = {
            "enable_formula": True,
            "language": "auto",
            "enable_table": True,
            "files": files_info
        }
        
        try:
            response = self.session.post(
                batch_url, 
                headers=headers, 
                json=data,
                timeout=(30, 60)
            )
            if response.status_code == 200:
                result = response.json()
                logger.info(f'âœ… æ‰¹é‡è¯·æ±‚å“åº”: {result}')
                
                if result["code"] == 0:
                    batch_id = result["data"]["batch_id"]
                    urls = result["data"]["file_urls"]
                    logger.info(f'ğŸ“¦ æ‰¹é‡ID: {batch_id}')
                    logger.info(f'ğŸ”— ä¸Šä¼ é“¾æ¥: {urls}')
                    
                    # ä¸Šä¼ æ–‡ä»¶åˆ°è¿”å›çš„URL
                    for i, url in enumerate(urls):
                        file_path = valid_files[i]
                        logger.info(f"ğŸ“¤ æ­£åœ¨ä¸Šä¼ : {file_path}")
                        try:
                            with open(file_path, 'rb') as f:
                                res_upload = self.session.put(
                                    url, 
                                    data=f,
                                    timeout=(30, 300)
                                )
                                if res_upload.status_code == 200:
                                    logger.info(f"âœ… {file_path} ä¸Šä¼ æˆåŠŸ")
                                else:
                                    logger.error(f"âŒ {file_path} ä¸Šä¼ å¤±è´¥, çŠ¶æ€ç : {res_upload.status_code}")
                        except Exception as upload_err:
                            logger.error(f"âŒ {file_path} ä¸Šä¼ è¿‡ç¨‹ä¸­å‡ºé”™: {upload_err}")
                        
                        # åœ¨æ–‡ä»¶ä¸Šä¼ ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…æœåŠ¡å™¨å‹åŠ›è¿‡å¤§
                        time.sleep(1)
                    
                    logger.info(f"âœ… æ‰¹é‡ä¸Šä¼ å®Œæˆï¼Œæ‰¹æ¬¡ID: {batch_id}")
                    
                    # ç­‰å¾…å¤„ç†å®Œæˆå¹¶ä¸‹è½½ç»“æœ
                    self.wait_and_download_batch_results(batch_id)
                    return batch_id
                else:
                    logger.error(f'âŒ ç”³è¯·ä¸Šä¼ URLå¤±è´¥: {result["msg"]}')
            else:
                logger.error(f'âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}')
                logger.error(f'å“åº”å†…å®¹: {response.text}')
        except Exception as err:
            logger.error(f"âŒ æ‰¹é‡å¤„ç†å‡ºé”™: {err}")
            
        return None

    def batch_process_folder(self, folder_path):
        """æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"""
        if not os.path.exists(folder_path):
            logger.error(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return
        
        # è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰çš„PDFå’ŒEMFæ–‡ä»¶
        supported_files = []
        temp_converted_files = []  # è®°å½•ä¸´æ—¶è½¬æ¢çš„æ–‡ä»¶ï¼Œä»¥ä¾¿åç»­æ¸…ç†
        
        for file_name in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file_name)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„æ–‡ä»¶æ ¼å¼
            if file_name.lower().endswith(('.pdf', '.png','.jpg')):
                supported_files.append(file_path)
            elif file_name.lower().endswith('.emf'):
                # å¯¹äºEMFæ–‡ä»¶ï¼Œå…ˆè½¬æ¢å†æ·»åŠ 
                converted_file = self.convert_emf_file(file_path)
                if converted_file:
                    supported_files.append(converted_file)
                    temp_converted_files.append(converted_file)  # è®°å½•ä»¥ä¾¿æ¸…ç†
        
        if not supported_files:
            logger.error(f"âŒ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶: {folder_path}")
            return
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(supported_files)} ä¸ªæ”¯æŒçš„æ–‡ä»¶ï¼Œå‡†å¤‡æ‰¹é‡å¤„ç†")
        
        try:
            # è°ƒç”¨æ‰¹é‡å¤„ç†æ–¹æ³•
            result = self.batch_process_pdfs(supported_files)
            return result
        finally:
            # æ¸…ç†ä¸´æ—¶è½¬æ¢çš„æ–‡ä»¶
            self.cleanup_temp_files(temp_converted_files)
    
    def convert_emf_file(self, emf_path):
        """
        è½¬æ¢EMFæ–‡ä»¶ä¸ºPNGæˆ–PDFï¼ˆä¼˜å…ˆè½¬æ¢ä¸ºPNGï¼‰
        
        Args:
            emf_path (str): EMFæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: è½¬æ¢åçš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            base_name = os.path.splitext(os.path.basename(emf_path))[0]
            folder_path = os.path.dirname(emf_path)
            
            # ä¼˜å…ˆå°è¯•è½¬æ¢ä¸ºPNGï¼ˆæ›´ç®€å•ç›´æ¥ï¼‰
            png_path = os.path.join(folder_path, f"{base_name}.png")
            if self.convert_emf_to_png(emf_path, png_path):
                return png_path
            
            # å¦‚æœPNGè½¬æ¢å¤±è´¥ï¼Œå°è¯•è½¬æ¢ä¸ºPDF
            pdf_path = os.path.join(folder_path, f"{base_name}.pdf")
            if self.convert_emf_to_pdf(emf_path, pdf_path):
                return pdf_path
                
            logger.error(f"âŒ æ— æ³•è½¬æ¢EMFæ–‡ä»¶: {emf_path}")
            return None
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢EMFæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None
    
    def cleanup_temp_files(self, temp_files):
        """
        æ¸…ç†ä¸´æ—¶è½¬æ¢çš„æ–‡ä»¶
        
        Args:
            temp_files (list): ä¸´æ—¶æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        for file_path in temp_files:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logger.info(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.error(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def check_batch_status(self, batch_id):
        """æ£€æŸ¥æ‰¹æ¬¡å¤„ç†çŠ¶æ€"""
        try:
            url = f'https://mineru.net/api/v4/extract-results/batch/{batch_id}'
            response = self.session.get(
                url, 
                headers=self.headers,
                timeout=(30, 60)
            )
            
            if response.status_code == 200:
                return response.json()["data"]
            else:
                logger.error(f"æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ‰¹æ¬¡çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def wait_and_download_batch_results(self, batch_id, check_interval=30):
        """
        ç­‰å¾…æ‰¹æ¬¡å¤„ç†å®Œæˆå¹¶ä¸‹è½½ç»“æœ
        
        Args:
            batch_id (str): æ‰¹æ¬¡ID
            check_interval (int): æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        logger.info(f"â³ ç­‰å¾…æ‰¹æ¬¡ {batch_id} å¤„ç†å®Œæˆ...")
        
        while True:
            # è·å–å½“å‰æ‰¹æ¬¡çŠ¶æ€
            data = self.check_batch_status(batch_id)
            if not data:
                logger.error(f"âŒ æ— æ³•è·å–æ‰¹æ¬¡ {batch_id} çŠ¶æ€")
                return
            
            # ç»Ÿè®¡å¤„ç†è¿›åº¦
            extract_results = data.get('extract_result', [])
            done_count = sum(1 for item in extract_results if item.get('state') == 'done')
            failed_count = sum(1 for item in extract_results if item.get('state') == 'failed')
            total_count = len(extract_results)
            
            logger.info(f"ğŸ“Š å¤„ç†ä¸­: {done_count}/{total_count} å·²å®Œæˆ, {failed_count} å¤±è´¥")
            
            # å¦‚æœæ‰€æœ‰æ–‡ä»¶éƒ½å¤„ç†å®Œæ¯•ï¼ˆå®Œæˆæˆ–å¤±è´¥ï¼‰ï¼Œåˆ™é€€å‡ºå¾ªç¯
            if done_count + failed_count == total_count:
                logger.info("âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå¼€å§‹ä¸‹è½½ç»“æœ...")
                self.download_batch_results(data)
                return
            
            # ç­‰å¾…æŒ‡å®šæ—¶é—´åå†æ¬¡æ£€æŸ¥
            time.sleep(check_interval)

    def extract_and_cleanup_zip(self, zip_path, output_dir=None):
        """
        è§£å‹zipæ–‡ä»¶åˆ°åŒåæ–‡ä»¶å¤¹å¹¶åˆ é™¤åŸæ–‡ä»¶
        
        Args:
            zip_path (str): zipæ–‡ä»¶è·¯å¾„
            output_dir (str): è§£å‹æ ¹ç›®å½•
        """
        if output_dir is None:
            output_dir = os.path.join(self.pdf_folder, "batch_results")
        try:
            # åˆ›å»ºä¸zipæ–‡ä»¶åŒåçš„æ–‡ä»¶å¤¹
            zip_name = os.path.splitext(os.path.basename(zip_path))[0]
            extract_folder = os.path.join(output_dir, zip_name)
            os.makedirs(extract_folder, exist_ok=True)
            
            # è§£å‹zipæ–‡ä»¶åˆ°åŒåæ–‡ä»¶å¤¹
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_folder)
            logger.info(f"âœ… å·²è§£å‹åˆ°æ–‡ä»¶å¤¹: {extract_folder}")
            
            # åˆ é™¤zipæ–‡ä»¶
            os.remove(zip_path)
            logger.info(f"ğŸ§¹ å·²åˆ é™¤: {zip_path}")
            
        except zipfile.BadZipFile as e:
            logger.error(f"âŒ ZIPæ–‡ä»¶æŸå {zip_path}: {str(e)}")
            # å°è¯•åˆ é™¤æŸåçš„æ–‡ä»¶
            try:
                os.remove(zip_path)
                logger.info(f"ğŸ§¹ å·²åˆ é™¤æŸåçš„æ–‡ä»¶: {zip_path}")
            except:
                pass
        except Exception as e:
            logger.error(f"âŒ è§£å‹æˆ–åˆ é™¤å¤±è´¥ {zip_path}: {str(e)}")

    def download_batch_results(self, batch_data):
        """
        ä¸‹è½½æ‰¹æ¬¡å¤„ç†ç»“æœ
        
        Args:
            batch_data (dict): æ‰¹æ¬¡å¤„ç†ç»“æœæ•°æ®
        """
        extract_results = batch_data.get('extract_result', [])
        success_count = 0
        
        # åˆ›å»ºç»“æœç›®å½•ï¼ˆå§‹ç»ˆåœ¨pdf_folderä¸‹ï¼‰
        output_dir = os.path.join(self.pdf_folder, "batch_results")
        os.makedirs(output_dir, exist_ok=True)
        
        for item in extract_results:
            if item.get('state') == 'done' and 'full_zip_url' in item:
                zip_url = item['full_zip_url']
                original_name = item['file_name']
                
                try:
                    # å®‰å…¨åœ°è·å–data_idï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ›¿ä»£
                    data_id = item.get('data_id')
                    if not data_id:
                        # å¦‚æœæ²¡æœ‰data_idï¼Œä½¿ç”¨æ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ä½œä¸ºæ ‡è¯†
                        data_id = os.path.splitext(original_name)[0]
                        logger.warning(f"âš ï¸ æ–‡ä»¶ {original_name} ç¼ºå°‘data_idï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºæ›¿ä»£: {data_id}")
                    
                    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
                    base_name = os.path.splitext(original_name)[0]
                    final_filename = f"{base_name}.md"
                    output_path = os.path.join(output_dir, final_filename)
                    
                    # ä¸‹è½½ZIPæ–‡ä»¶ï¼Œä½¿ç”¨data_idæˆ–æ›¿ä»£æ ‡è¯†ä½œä¸ºæ–‡ä»¶å
                    logger.info(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {original_name}")
                    
                    # å°è¯•HTTPSä¸‹è½½
                    try:
                        zip_response = self.session.get(
                            zip_url, 
                            stream=True,
                            timeout=(30, 300)
                        )
                        zip_response.raise_for_status()
                    except requests.exceptions.SSLError:
                        # å¦‚æœHTTPSå¤±è´¥ï¼Œå°è¯•HTTP
                        logger.warning(f"âš ï¸ HTTPSä¸‹è½½å¤±è´¥ï¼Œå°è¯•HTTP...")
                        http_url = zip_url.replace('https://', 'http://')
                        zip_response = self.session.get(
                            http_url, 
                            stream=True,
                            timeout=(30, 300)
                        )
                        zip_response.raise_for_status()
                    
                    # ä¿å­˜ZIPæ–‡ä»¶
                    zip_path = os.path.join(output_dir, f"{data_id}.zip")
                    with open(zip_path, 'wb') as f:
                        for chunk in zip_response.iter_content(1024 * 1024):
                            if chunk:  # è¿‡æ»¤æ‰keep-alive chunks
                                f.write(chunk)
                    
                    logger.info(f"âœ… {original_name} ä¸‹è½½å®Œæˆ")
                    
                    # è§£å‹å¹¶åˆ é™¤zipæ–‡ä»¶
                    self.extract_and_cleanup_zip(zip_path, output_dir)
                    success_count += 1
                    
                except Exception as e:
                    logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {original_name} | é”™è¯¯: {str(e)}")
        
        logger.info(f"ğŸ æ‰¹é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸä¸‹è½½ {success_count}/{len(extract_results)} ä¸ªæ–‡ä»¶")
        
        # æ·»åŠ æ–‡æœ¬æ•°æ®åˆ°åŸå§‹image_mapping.jsonæ–‡ä»¶
        if self.pdf_folder and os.path.exists(self.pdf_folder):
            mapping_file_path = os.path.join(self.pdf_folder, "image_mapping.json")
            if os.path.exists(mapping_file_path):
                self.collect_and_merge_text_to_mapping(mapping_file_path, output_dir)
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ˜ å°„æ–‡ä»¶: {mapping_file_path}")
        else:
            logger.warning("âš ï¸ æœªè®¾ç½®pdf_folderæˆ–æ–‡ä»¶å¤¹ä¸å­˜åœ¨")

    def collect_and_merge_text_to_mapping(self, source_mapping_file, batch_results_dir=None):
        """
        æ”¶é›†å¤„ç†åçš„æ–‡æœ¬æ•°æ®å¹¶åˆå¹¶åˆ°åŸå§‹çš„image_mapping.jsonæ–‡ä»¶ä¸­
        
        Args:
            source_mapping_file (str): åŸå§‹image_mapping.jsonæ–‡ä»¶è·¯å¾„
            batch_results_dir (str): æ‰¹å¤„ç†ç»“æœç›®å½•
        """
        if batch_results_dir is None:
            batch_results_dir = os.path.join(self.pdf_folder, "batch_results")
        logger.info(f"ğŸ” å¼€å§‹æ”¶é›†æ–‡æœ¬æ•°æ®...")
        logger.info(f"ğŸ“ æ‰¹å¤„ç†ç»“æœç›®å½•: {batch_results_dir}")
        logger.info(f"ğŸ“„ æ˜ å°„æ–‡ä»¶è·¯å¾„: {source_mapping_file}")
        
        # æ£€æŸ¥ç»“æœç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(batch_results_dir):
            logger.error(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {batch_results_dir}")
            return
        
        # è¯»å–åŸå§‹image_mapping.jsonæ–‡ä»¶
        try:
            with open(source_mapping_file, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
            logger.info(f"âœ… æˆåŠŸè¯»å–æ˜ å°„æ–‡ä»¶")
        except Exception as e:
            logger.error(f"âŒ æ— æ³•è¯»å–åŸå§‹æ˜ å°„æ–‡ä»¶ {source_mapping_file}: {e}")
            return
        
        # éå†å¤„ç†ç»“æœç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶å¤¹ï¼Œæ”¶é›†æ–‡æœ¬æ•°æ®
        text_mapping = {}
        
        logger.info(f"ğŸ“‚ æŸ¥æ‰¾ç»“æœæ–‡ä»¶å¤¹ä¸­çš„content_list.jsonæ–‡ä»¶...")
        for folder_name in os.listdir(batch_results_dir):
            folder_path = os.path.join(batch_results_dir, folder_name)
            
            # ç¡®ä¿æ˜¯ç›®å½•
            if not os.path.isdir(folder_path):
                continue
                
            logger.info(f"ğŸ“ æ£€æŸ¥æ–‡ä»¶å¤¹: {folder_name}")
            
            # æŸ¥æ‰¾ç”Ÿæˆçš„content_list.jsonæ–‡ä»¶ï¼ˆå¤„ç†ç»“æœï¼‰
            content_list_json_path = None
            for file in os.listdir(folder_path):
                if file.endswith('_content_list.json'):
                    content_list_json_path = os.path.join(folder_path, file)
                    logger.info(f"ğŸ“„ æ‰¾åˆ°content_listæ–‡ä»¶: {file}")
                    break
            
            if not content_list_json_path or not os.path.exists(content_list_json_path):
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å¤„ç†åçš„content_list.jsonæ–‡ä»¶: {folder_name}")
                continue
                
            # è¯»å–å¤„ç†åçš„JSONæ–‡ä»¶
            try:
                with open(content_list_json_path, 'r', encoding='utf-8') as f:
                    content_data = json.load(f)
                logger.info(f"âœ… æˆåŠŸè¯»å– {content_list_json_path}")
            except Exception as e:
                logger.error(f"âŒ æ— æ³•è¯»å–å¤„ç†åçš„JSONæ–‡ä»¶ {content_list_json_path}: {e}")
                continue
                
            # æå–æ–‡æœ¬å†…å®¹
            all_text = self._extract_text_from_content_list(content_data)
            
            if not all_text:
                logger.warning(f"âš ï¸ ä» {folder_name} ä¸­æœªæå–åˆ°æ–‡æœ¬å†…å®¹")
                continue
                
            # ä½¿ç”¨æ–‡ä»¶å¤¹åä½œä¸ºé”®å­˜å‚¨æ–‡æœ¬æ•°æ®
            text_mapping[folder_name] = all_text
            logger.info(f"âœ… å·²æå– {folder_name} çš„æ–‡æœ¬æ•°æ®ï¼Œå…± {len(all_text)} æ¡æ–‡æœ¬")
        
        logger.info(f"ğŸ“Š å…±æ”¶é›†åˆ° {len(text_mapping)} ä¸ªæ–‡ä»¶çš„æ–‡æœ¬æ•°æ®")
        
        # å°†æ–‡æœ¬æ•°æ®æ˜ å°„åˆ°image_mapping.jsonä¸­
        updated_count = 0
        logger.info(f"ğŸ”„ å¼€å§‹å°†æ–‡æœ¬æ•°æ®æ˜ å°„åˆ°image_mapping.jsonä¸­...")
        
        for slide_key, slide_data in mapping_data.items():
            if 'images' in slide_data:
                for image_info in slide_data['images']:
                    filename = image_info.get('filename')
                    if filename:
                        # ä¸å†ç§»é™¤æ–‡ä»¶æ‰©å±•åï¼Œè€Œæ˜¯å°è¯•å¤šç§åŒ¹é…æ–¹å¼
                        name_without_ext = os.path.splitext(filename)[0]
                        logger.info(f"ğŸ” åŒ¹é…æ–‡ä»¶å: {filename} -> å°è¯•åŒ¹é… {filename} æˆ– {name_without_ext}")
                        
                        # å°è¯•å¤šç§åŒ¹é…æ–¹å¼
                        matched = False
                        # æ–¹å¼1: å®Œå…¨åŒ¹é…ï¼ˆåŒ…å«æ‰©å±•åï¼‰
                        if filename in text_mapping:
                            image_info['all_text'] = text_mapping[filename]
                            updated_count += 1
                            matched = True
                            logger.info(f"âœ… é€šè¿‡å®Œæ•´æ–‡ä»¶å {filename} åŒ¹é…æˆåŠŸ")
                        # æ–¹å¼2: å»æ‰æ‰©å±•ååŒ¹é…
                        elif name_without_ext in text_mapping:
                            image_info['all_text'] = text_mapping[name_without_ext]
                            updated_count += 1
                            matched = True
                            logger.info(f"âœ… é€šè¿‡æ–‡ä»¶å(æ— æ‰©å±•å) {name_without_ext} åŒ¹é…æˆåŠŸ")
                        # æ–¹å¼3: å¦‚æœç»“æœæ–‡ä»¶å¤¹åä¸filenameå®Œå…¨ä¸€è‡´
                        else:
                            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸filenameå®Œå…¨ä¸€è‡´çš„æ–‡ä»¶å¤¹
                            full_path = os.path.join(batch_results_dir, filename)
                            if os.path.exists(full_path) and os.path.isdir(full_path):
                                # åœ¨è¯¥æ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾content_list.json
                                content_list_path = None
                                for file in os.listdir(full_path):
                                    if file.endswith('_content_list.json'):
                                        content_list_path = os.path.join(full_path, file)
                                        break
                                
                                if content_list_path and os.path.exists(content_list_path):
                                    try:
                                        with open(content_list_path, 'r', encoding='utf-8') as f:
                                            content_data = json.load(f)
                                        all_text = self._extract_text_from_content_list(content_data)
                                        if all_text:
                                            image_info['all_text'] = all_text
                                            updated_count += 1
                                            matched = True
                                            logger.info(f"âœ… é€šè¿‡ç›®å½•åŒ¹é… {filename} æˆåŠŸ")
                                    except Exception as e:
                                        logger.error(f"âŒ è¯»å– {content_list_path} æ—¶å‡ºé”™: {e}")
                        
                        if not matched:
                            logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {filename} å¯¹åº”çš„æ–‡æœ¬æ•°æ®")
        
        logger.info(f"ğŸ“ å…±æ›´æ–°äº† {updated_count} ä¸ªå›¾åƒæ¡ç›®")
        
        # å°†æ›´æ–°åçš„æ•°æ®å†™å›image_mapping.json
        try:
            with open(source_mapping_file, 'w', encoding='utf-8') as f:
                json.dump(mapping_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… æˆåŠŸæ›´æ–° {updated_count} ä¸ªå›¾åƒæ¡ç›®ï¼Œæ–‡æœ¬æ•°æ®å·²æ·»åŠ åˆ° {source_mapping_file}")
        except Exception as e:
            logger.error(f"âŒ æ— æ³•å†™å…¥æ›´æ–°åçš„æ•°æ®åˆ° {source_mapping_file}: {e}")

    def _extract_text_from_content_list(self, content_data):
        """
        ä»content_listæ•°æ®ä¸­æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
        
        Args:
            content_data (list): content_listæ•°æ®
            
        Returns:
            dict: åŒ…å«æ‰€æœ‰æ–‡æœ¬çš„å­—å…¸ï¼Œæ ¼å¼ä¸º {"text1": "...", "text2": "..."}
        """
        texts = []
        
        # éå†content_listä¸­çš„æ‰€æœ‰æ¡ç›®
        for item in content_data:
            if isinstance(item, dict) and item.get('type') == 'text' and 'text' in item:
                text_value = item['text']
                if isinstance(text_value, str) and text_value.strip():
                    texts.append(text_value.strip())
        
        # æ„é€ all_textå­—å…¸
        all_text = {}
        for i, text in enumerate(texts, 1):
            all_text[f"text{i}"] = text
            
        return all_text


#api token 14å¤©ä¸€æ¢
TOKEN = os.getenv("OCR_TOKEN")

# å¤„ç†æ–‡ä»¶å¤¹ï¼ˆç°åœ¨ä¹Ÿæ”¯æŒEMFæ–‡ä»¶ï¼‰
pdf_folder = "/tmp/ppt_ocr_0xq_xja2" # ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶å¤¹è·¯å¾„
processor = OCRProcessor(TOKEN, pdf_folder)
processor.batch_process_folder(pdf_folder)